import { beforeAll, describe, expect, test } from 'vitest';
import type { Morpheme } from '../../src/core/morpheme.js';
import { SplitMode } from '../../src/core/tokenizer.js';
import {
	createTokenizer,
	getSystemDictionary,
	getUser1Dictionary,
} from './testDictionary.js';

describe('JapaneseTokenizer - Basic Tokenization', () => {
	let tokenizer: Awaited<ReturnType<typeof createTokenizer>>;

	beforeAll(async () => {
		tokenizer = await createTokenizer(0);
	});

	test('tokenizeSmallKatakanaOnly - single small katakana character', () => {
		const result = tokenizer.tokenize(SplitMode.C, '„Ç°');
		expect(result.size()).toBe(1);
		const morpheme = result.get(0);
		expect(morpheme).toBeDefined();
		expect(morpheme!.surface()).toBe('„Ç°');
	});

	test('partOfSpeech - verify POS retrieval', () => {
		const result = tokenizer.tokenize(SplitMode.C, '‰∫¨ÈÉΩ');
		expect(result.size()).toBe(1);
		const morpheme = result.get(0);
		expect(morpheme).toBeDefined();
		const pos = morpheme!.partOfSpeech();
		expect(pos).toEqual(['ÂêçË©û', 'Âõ∫ÊúâÂêçË©û', 'Âú∞Âêç', '‰∏ÄËà¨', '*', '*']);
	});

	test('tokenizeKanjiAlphabetWord - mixed kanji/alpha', () => {
		const result = tokenizer.tokenize(SplitMode.C, 'Áâπa');
		expect(result.size()).toBe(1);
		const morpheme = result.get(0);
		expect(morpheme).toBeDefined();
		expect(morpheme!.surface()).toBe('Áâπa');
	});
});

describe('JapaneseTokenizer - Word ID and Dictionary ID', () => {
	test('getWordId - verify word ID uniqueness', async () => {
		const dict = await getSystemDictionary();
		// First tokenize to get the actual word IDs
		const result = dict.create().tokenize(SplitMode.C, '‰∫¨ÈÉΩ');
		expect(result.size()).toBe(1);
		const wid = result.get(0)!.getWordId();
		expect(wid).toBeGreaterThanOrEqual(0);

		// Now use getWordId to find the same word
		// The word "‰∫¨ÈÉΩ" has POS "ÂêçË©û,Âõ∫ÊúâÂêçË©û,Âú∞Âêç,‰∏ÄËà¨,*,*" which is POS ID 3
		const wordId = dict.getLexicon().getWordId('‰∫¨ÈÉΩ', 3, '„Ç≠„Éß„Ç¶„Éà');
		expect(wordId).toBe(wid);
	});

	test('getDictionaryId - system dictionary returns 0', async () => {
		const dict = await getSystemDictionary();
		// The word "‰∫¨ÈÉΩ" has POS ID 3 (ÂêçË©û,Âõ∫ÊúâÂêçË©û,Âú∞Âêç,‰∏ÄËà¨,*,*)
		// and reading form "„Ç≠„Éß„Ç¶„Éà"
		// This should be found as word ID 3 (or higher) in system dictionary
		const wordId = dict.getLexicon().getWordId('‰∫¨ÈÉΩ', 3, '„Ç≠„Éß„Ç¶„Éà');
		expect(wordId).toBeGreaterThanOrEqual(0);
	});
});

describe('JapaneseTokenizer - Synonym Group IDs', () => {
	test('getSynonymGroupIds - verify synonym groups', async () => {
		const dict = await getSystemDictionary();
		// The word "‰∫¨ÈÉΩ" has POS ID 3 (ÂêçË©û,Âõ∫ÊúâÂêçË©û,Âú∞Âêç,‰∏ÄËà¨,*,*)
		// and reading form "„Ç≠„Éß„Ç¶„Éà"
		const kyotoId = dict.getLexicon().getWordId('‰∫¨ÈÉΩ', 3, '„Ç≠„Éß„Ç¶„Éà');
		expect(kyotoId).toBeGreaterThanOrEqual(0);
		const wordInfo = dict.getLexicon().getWordInfo(kyotoId);
		expect(wordInfo.getSynonymGroupIds().length).toBeGreaterThan(0);
	});
});

describe('JapaneseTokenizer - Sentence Tokenization', () => {
	let tokenizer: Awaited<ReturnType<typeof createTokenizer>>;

	beforeAll(async () => {
		tokenizer = await createTokenizer(0);
	});

	async function* _toAsyncIterable(
		data: string[] | string,
	): AsyncIterable<string> {
		if (typeof data === 'string') {
			yield data;
		} else {
			yield* data;
		}
	}

	test('tokenizeSentences - multi-sentence with Japanese period', () => {
		const text = '‰∫¨ÈÉΩ„Å´Ë°å„Å£„Åü„ÄÇÊù±‰∫¨„Å´Ë°å„Å£„Åü„ÄÇ';
		const sentences = tokenizer.tokenizeSentences(SplitMode.C, text);
		const list = [...sentences];

		expect(list.length).toBe(2);

		const first = list[0];
		expect(first).toBeDefined();
		expect([...first!].length).toBeGreaterThan(0);
		const firstMorpheme = first!.get(0);
		expect(firstMorpheme).toBeDefined();
		expect(firstMorpheme!.surface()).toBe('‰∫¨ÈÉΩ');

		const second = list[1];
		expect(second).toBeDefined();
		expect([...second!].length).toBeGreaterThan(0);
		const secondMorpheme = second!.get(0);
		expect(secondMorpheme).toBeDefined();
		expect(secondMorpheme!.surface()).toBe('Êù±‰∫¨');
	});

	test('tokenizerWithDots - ellipsis handling', () => {
		const result = tokenizer.tokenize(SplitMode.C, '‚Ä¶‚Ä¶');
		expect(result.size()).toBeGreaterThanOrEqual(1);
	});

	test('tokenizeSentencesWithSurrogatePair - emoji handling', () => {
		const result = tokenizer.tokenize(SplitMode.C, 'üòÄ');
		expect(result.size()).toBeGreaterThanOrEqual(1);
	});

	test('tokenizeSentences - skips leading newline tokens between sentences', () => {
		const text = '‰∫¨ÈÉΩ„Å´Ë°å„Å£„Åü„ÄÇ\nÊù±‰∫¨„Å´Ë°å„Å£„Åü„ÄÇ';
		const list = [...tokenizer.tokenizeSentences(SplitMode.C, text)];

		expect(list.length).toBe(2);
		const secondSurfaces = [...list[1]!].map((m) => m.surface());
		expect(secondSurfaces[0]).toBe('Êù±‰∫¨');
		expect(secondSurfaces).not.toContain('\n');
	});

	test('tokenizeSentences - splits quoted dialogue before following narration', () => {
		const text = '„Äå„Éã„É£ÔºÅ„Äç\n„Çø„Éû„ÅØÈ£ü„Åπ„Åü„ÄÇ';
		const list = [...tokenizer.tokenizeSentences(SplitMode.C, text)];

		expect(list.length).toBe(2);
		expect(list[0]?.get(0)?.surface()).toBe('„Äå');
		const secondSurfaces = [...list[1]!].map((m) => m.surface());
		expect(secondSurfaces.slice(0, 2).join('')).toBe('„Çø„Éû');
		expect(secondSurfaces).not.toContain('\n');
	});
});

describe('JapaneseTokenizer - Lazy Tokenization', () => {
	let tokenizer: Awaited<ReturnType<typeof createTokenizer>>;

	beforeAll(async () => {
		tokenizer = await createTokenizer(0);
	});

	async function* toAsyncIterable(
		data: string[] | string,
	): AsyncIterable<string> {
		if (typeof data === 'string') {
			yield data;
		} else {
			yield* data;
		}
	}

	test('lazyTokenizeSentences - streaming iteration', async () => {
		const _text = '‰∫¨ÈÉΩ„Å´Ë°å„Å£„Åü„ÄÇÊù±‰∫¨„Å´Ë°å„Å£„Åü„ÄÇ';
		const chunks: Morpheme[][] = [];

		for await (const sentence of tokenizer.lazyTokenizeSentences(
			SplitMode.C,
			toAsyncIterable(['‰∫¨ÈÉΩ„Å´Ë°å„Å£„Åü„ÄÇ', 'Êù±‰∫¨„Å´Ë°å„Å£„Åü„ÄÇ']),
		)) {
			chunks.push([...sentence]);
		}

		expect(chunks.length).toBe(2);
		expect(chunks[0]?.length).toBeGreaterThan(0);
		expect(chunks[0]?.[0]?.surface()).toBe('‰∫¨ÈÉΩ');
		expect(chunks[1]?.length).toBeGreaterThan(0);
		expect(chunks[1]?.[0]?.surface()).toBe('Êù±‰∫¨');
	});

	test('lazyTokenizeSentencesWithLongText - large text streaming', async () => {
		const text = '‰∫¨ÈÉΩ„Å´'.repeat(1000);
		const chunks: Morpheme[][] = [];

		for await (const sentence of tokenizer.lazyTokenizeSentences(
			SplitMode.C,
			toAsyncIterable(text),
		)) {
			chunks.push([...sentence]);
		}

		expect(chunks.length).toBeGreaterThanOrEqual(1);
		expect(chunks[0]?.length).toBeGreaterThan(0);
	});

	test('lazyTokenizeSentences - skips leading newline tokens between sentences', async () => {
		const chunks: Morpheme[][] = [];

		for await (const sentence of tokenizer.lazyTokenizeSentences(
			SplitMode.C,
			toAsyncIterable('‰∫¨ÈÉΩ„Å´Ë°å„Å£„Åü„ÄÇ\nÊù±‰∫¨„Å´Ë°å„Å£„Åü„ÄÇ'),
		)) {
			chunks.push([...sentence]);
		}

		expect(chunks.length).toBe(2);
		const secondSurfaces = chunks[1]?.map((m) => m.surface()) ?? [];
		expect(secondSurfaces[0]).toBe('Êù±‰∫¨');
		expect(secondSurfaces).not.toContain('\n');
	});
});

describe('JapaneseTokenizer - Split Modes', () => {
	let tokenizer: Awaited<ReturnType<typeof createTokenizer>>;

	beforeAll(async () => {
		tokenizer = await createTokenizer(0);
	});

	test('splitAfterTokenizeCtoA - mode C to A split', () => {
		const resultC = tokenizer.tokenize(SplitMode.C, 'Êù±‰∫¨ÈÉΩ');
		const resultA = tokenizer.tokenize(SplitMode.A, 'Êù±‰∫¨ÈÉΩ');

		expect(resultC.size()).toBeGreaterThanOrEqual(1);
		expect(resultA.size()).toBeGreaterThanOrEqual(1);

		const surfaceC = resultC.get(0)?.surface() ?? '';
		const surfaceA = resultA.get(0)?.surface() ?? '';

		expect(surfaceC).toBe('Êù±‰∫¨ÈÉΩ');
		expect(surfaceA).toBe('Êù±‰∫¨');
	});

	test('splitAfterTokenizeCtoB - mode C to B split', () => {
		const resultC = tokenizer.tokenize(SplitMode.C, 'Êù±‰∫¨ÈÉΩ');
		const resultB = tokenizer.tokenize(SplitMode.B, 'Êù±‰∫¨ÈÉΩ');

		expect(resultC.size()).toBeGreaterThanOrEqual(1);
		expect(resultB.size()).toBeGreaterThanOrEqual(1);

		const surfaceC = resultC.get(0)?.surface() ?? '';
		const surfaceB = resultB.get(0)?.surface() ?? '';

		expect(surfaceC).toBe('Êù±‰∫¨ÈÉΩ');
		expect(surfaceB).toBe('Êù±‰∫¨ÈÉΩ');
	});

	test('splitSingleToken - single token morpheme', () => {
		const result = tokenizer.tokenize(SplitMode.A, '‰∫¨ÈÉΩ');
		expect(result.size()).toBe(1);
		expect(result.get(0)?.surface()).toBe('‰∫¨ÈÉΩ');
	});
});

describe('JapaneseTokenizer - Morpheme Properties', () => {
	let tokenizer: Awaited<ReturnType<typeof createTokenizer>>;

	beforeAll(async () => {
		tokenizer = await createTokenizer(0);
	});

	test('morpheme surface', () => {
		const result = tokenizer.tokenize(SplitMode.C, '‰∫¨ÈÉΩ');
		expect(result.size()).toBe(1);
		expect(result.get(0)?.surface()).toBe('‰∫¨ÈÉΩ');
	});

	test('morpheme begin and end', () => {
		const result = tokenizer.tokenize(SplitMode.C, '‰∫¨ÈÉΩ');
		expect(result.size()).toBe(1);
		const m = result.get(0);
		expect(m).toBeDefined();
		expect(m!.begin()).toBe(0);
		expect(m!.end()).toBe(2);
	});

	test('morpheme normalized form', () => {
		const result = tokenizer.tokenize(SplitMode.C, '‰∫¨ÈÉΩ');
		expect(result.size()).toBe(1);
		const m = result.get(0);
		expect(m).toBeDefined();
		expect(m!.normalizedForm()).toBe('‰∫¨ÈÉΩ');
	});

	test('morpheme dictionary word ID', () => {
		const result = tokenizer.tokenize(SplitMode.C, '‰∫¨ÈÉΩ');
		expect(result.size()).toBe(1);
		const m = result.get(0);
		expect(m).toBeDefined();
		expect(m!.getWordId()).toBeGreaterThanOrEqual(0);
	});
});

describe('JapaneseTokenizer - Lattice Dump', () => {
	let tokenizer: Awaited<ReturnType<typeof createTokenizer>>;

	beforeAll(async () => {
		tokenizer = await createTokenizer(0);
	});

	test('dumpInternalStructures - verify lattice structure JSON', () => {
		const dump = tokenizer.dumpInternalStructures('Êù±‰∫¨ÈÉΩ');

		expect(dump).toBeTruthy();
		expect(typeof dump).toBe('string');

		const parsed = JSON.parse(dump);
		expect(parsed).toBeDefined();
		expect(parsed.nodes).toBeDefined();
		expect(parsed.nodes.length).toBeGreaterThanOrEqual(1);
		expect(parsed.bestPath).toBeDefined();
		expect(parsed.bestPath.length).toBeGreaterThanOrEqual(1);
	});

	test('dumpInternalStructures -Êù±‰∫¨ÈÉΩ produces expected lattice', () => {
		const dump = tokenizer.dumpInternalStructures('Êù±‰∫¨ÈÉΩ');
		const parsed = JSON.parse(dump);

		expect(parsed.nodes.length).toBeGreaterThanOrEqual(5);
		expect(parsed.bestPath.length).toBe(1);
		expect(parsed.rewrittenPath).toBeDefined();
	});
});

describe('JapaneseTokenizer - User Dictionary', () => {
	test('user dictionary - custom word in user dict', async () => {
		const user1 = await getUser1Dictionary();
		const tokenizer = user1.create();

		const result = tokenizer.tokenize(SplitMode.C, '„Å¥„Çâ„Çã');
		expect(result.size()).toBe(1);
		expect(result.get(0)?.surface()).toBe('„Å¥„Çâ„Çã');

		const dictId = result.get(0)?.getDictionaryId();
		expect(dictId).toBe(1);
	});

	test('user dictionary - word in system dict', async () => {
		const user1 = await getUser1Dictionary();
		const tokenizer = user1.create();

		const result = tokenizer.tokenize(SplitMode.C, '‰∫¨ÈÉΩ');
		expect(result.size()).toBe(1);
		expect(result.get(0)?.surface()).toBe('‰∫¨ÈÉΩ');

		const dictId = result.get(0)?.getDictionaryId();
		expect(dictId).toBe(0);
	});
});

describe('JapaneseTokenizer - Numeric Handling', () => {
	let tokenizer: Awaited<ReturnType<typeof createTokenizer>>;

	beforeAll(async () => {
		tokenizer = await createTokenizer(0);
	});

	test('tokenize Arabic numerals', () => {
		const result = tokenizer.tokenize(SplitMode.C, '123');
		expect(result.size()).toBeGreaterThanOrEqual(1);
	});

	test('tokenize Kanji numerals', () => {
		const result = tokenizer.tokenize(SplitMode.C, '‰∏Ä‰∫å‰∏â');
		expect(result.size()).toBeGreaterThanOrEqual(1);
	});

	test('tokenize mixed numerals', () => {
		const result = tokenizer.tokenize(SplitMode.C, '1234567890');
		expect(result.size()).toBeGreaterThanOrEqual(1);
	});
});

describe('JapaneseTokenizer - Empty and Edge Cases', () => {
	let tokenizer: Awaited<ReturnType<typeof createTokenizer>>;

	beforeAll(async () => {
		tokenizer = await createTokenizer(0);
	});

	test('empty string returns empty list', () => {
		const result = tokenizer.tokenize(SplitMode.C, '');
		expect(result.size()).toBe(0);
	});

	test('unknown word becomes OOV', () => {
		const result = tokenizer.tokenize(SplitMode.C, 'xyzxyzxyz');
		expect(result.size()).toBeGreaterThanOrEqual(1);

		const m = result.get(0);
		expect(m).toBeDefined();
		expect(m!.getWordId()).toBe(-1);
	});

	test('newline between OOV runs keeps lattice connected', () => {
		const result = tokenizer.tokenize(SplitMode.C, '„Åß„Åô\n„Åß„Åô');
		const surfaces = [...result].map((m) => m.surface());

		expect(surfaces).toEqual(['„Åß', '„Åô', '\n', '„Åß', '„Åô']);
	});

	test('zero length morpheme from special processing', () => {
		const result = tokenizer.tokenize(SplitMode.C, '„Å™„ÄÇ„Å™');
		expect(result.size()).toBeGreaterThanOrEqual(1);
	});
});
